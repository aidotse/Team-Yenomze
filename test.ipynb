{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchio in /opt/conda/lib/python3.6/site-packages (0.17.51)\n",
      "Requirement already satisfied: Python-Deprecated in /opt/conda/lib/python3.6/site-packages (from torchio) (1.1.0)\n",
      "Requirement already satisfied: SimpleITK<2 in /opt/conda/lib/python3.6/site-packages (from torchio) (1.2.4)\n",
      "Requirement already satisfied: torchvision in /opt/conda/lib/python3.6/site-packages (from torchio) (0.8.0a0)\n",
      "Requirement already satisfied: Click in /opt/conda/lib/python3.6/site-packages (from torchio) (7.1.2)\n",
      "Requirement already satisfied: humanize in /opt/conda/lib/python3.6/site-packages (from torchio) (3.1.0)\n",
      "Requirement already satisfied: torch>=1.1 in /opt/conda/lib/python3.6/site-packages (from torchio) (1.7.0a0+7036e91)\n",
      "Requirement already satisfied: nibabel in /opt/conda/lib/python3.6/site-packages (from torchio) (3.2.0)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.6/site-packages (from torchio) (1.5.2)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.6/site-packages (from torchio) (1.19.1)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.6/site-packages (from torchio) (4.31.1)\n",
      "Requirement already satisfied: pillow>=4.1.1 in /opt/conda/lib/python3.6/site-packages (from torchvision->torchio) (8.0.1)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.6/site-packages (from humanize->torchio) (50.3.0.post20201006)\n",
      "Requirement already satisfied: future in /opt/conda/lib/python3.6/site-packages (from torch>=1.1->torchio) (0.18.2)\n",
      "Requirement already satisfied: typing_extensions in /opt/conda/lib/python3.6/site-packages (from torch>=1.1->torchio) (3.7.4.3)\n",
      "Requirement already satisfied: dataclasses in /opt/conda/lib/python3.6/site-packages (from torch>=1.1->torchio) (0.7)\n",
      "Requirement already satisfied: packaging>=14.3 in /opt/conda/lib/python3.6/site-packages (from nibabel->torchio) (20.4)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.6/site-packages (from packaging>=14.3->nibabel->torchio) (2.4.7)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.6/site-packages (from packaging>=14.3->nibabel->torchio) (1.15.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install torchio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: monai in /opt/conda/lib/python3.6/site-packages (0.3.0)\n",
      "Requirement already satisfied: torch>=1.4 in /opt/conda/lib/python3.6/site-packages (from monai) (1.7.0a0+7036e91)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.6/site-packages (from monai) (1.19.1)\n",
      "Requirement already satisfied: future in /opt/conda/lib/python3.6/site-packages (from torch>=1.4->monai) (0.18.2)\n",
      "Requirement already satisfied: typing_extensions in /opt/conda/lib/python3.6/site-packages (from torch>=1.4->monai) (3.7.4.3)\n",
      "Requirement already satisfied: dataclasses in /opt/conda/lib/python3.6/site-packages (from torch>=1.4->monai) (0.7)\n"
     ]
    }
   ],
   "source": [
    "!pip install monai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: itk in /opt/conda/lib/python3.6/site-packages (5.1.1.post1)\n",
      "Requirement already satisfied: itk-io==5.1.1.post1 in /opt/conda/lib/python3.6/site-packages (from itk) (5.1.1.post1)\n",
      "Requirement already satisfied: itk-core==5.1.1.post1 in /opt/conda/lib/python3.6/site-packages (from itk) (5.1.1.post1)\n",
      "Requirement already satisfied: itk-numerics==5.1.1.post1 in /opt/conda/lib/python3.6/site-packages (from itk) (5.1.1.post1)\n",
      "Requirement already satisfied: itk-filtering==5.1.1.post1 in /opt/conda/lib/python3.6/site-packages (from itk) (5.1.1.post1)\n",
      "Requirement already satisfied: itk-segmentation==5.1.1.post1 in /opt/conda/lib/python3.6/site-packages (from itk) (5.1.1.post1)\n",
      "Requirement already satisfied: itk-registration==5.1.1.post1 in /opt/conda/lib/python3.6/site-packages (from itk) (5.1.1.post1)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.6/site-packages (from itk) (1.19.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install itk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If you use TorchIO for your research, please cite the following paper:\n",
      "Pérez-García et al., TorchIO: a Python library for efficient loading,\n",
      "preprocessing, augmentation and patch-based sampling of medical images\n",
      "in deep learning. Credits instructions: https://torchio.readthedocs.io/#credits\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import glob\n",
    "import multiprocessing\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch import autograd\n",
    "\n",
    "from torchio.transforms import CropOrPad\n",
    "from monai.data import ArrayDataset, DataLoader, PILReader\n",
    "from monai.transforms import Compose, LoadImage, AddChannel, RandFlip, RandRotate, RandRotate90, RandScaleIntensity, CenterSpatialCrop, ToTensor, ScaleIntensity, LoadPNG, RandSpatialCrop\n",
    "from monai.visualize import plot_2d_or_3d_image\n",
    "\n",
    "import FlowArrayDataset\n",
    "\n",
    "from utils import *\n",
    "from VGGLoss import *\n",
    "from Generator import *\n",
    "from Discriminator import *\n",
    "import pytorch_ssim\n",
    "\n",
    "from math import log10\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.transforms import ToPILImage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"/data/*\"\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6 input slices\n",
    "inputZ01_path = sorted(glob.glob(os.path.join(data_dir, '*A04Z01*.tif'), recursive=True))\n",
    "inputZ02_path = sorted(glob.glob(os.path.join(data_dir, '*A04Z02*.tif'), recursive=True))\n",
    "inputZ03_path = sorted(glob.glob(os.path.join(data_dir, '*A04Z03*.tif'), recursive=True))\n",
    "inputZ04_path = sorted(glob.glob(os.path.join(data_dir, '*A04Z04*.tif'), recursive=True))\n",
    "inputZ05_path = sorted(glob.glob(os.path.join(data_dir, '*A04Z05*.tif'), recursive=True))\n",
    "inputZ06_path = sorted(glob.glob(os.path.join(data_dir, '*A04Z06*.tif'), recursive=True))\n",
    "inputZ07_path = sorted(glob.glob(os.path.join(data_dir, '*A04Z07*.tif'), recursive=True))\n",
    "\n",
    "# 3 output channels\n",
    "targetC01_path = sorted(glob.glob(os.path.join(data_dir, '*C01.tif'), recursive=True))\n",
    "targetC02_path = sorted(glob.glob(os.path.join(data_dir, '*C02.tif'), recursive=True))\n",
    "targetC03_path = sorted(glob.glob(os.path.join(data_dir, '*C03.tif'), recursive=True))\n",
    "\n",
    "# split training/validation\n",
    "inputZ01, inputZ01_val = split_train_val(inputZ01_path)\n",
    "inputZ02, inputZ02_val = split_train_val(inputZ02_path)\n",
    "inputZ03, inputZ03_val = split_train_val(inputZ03_path)\n",
    "inputZ04, inputZ04_val = split_train_val(inputZ04_path)\n",
    "inputZ05, inputZ05_val = split_train_val(inputZ05_path)\n",
    "inputZ06, inputZ06_val = split_train_val(inputZ06_path)\n",
    "inputZ07, inputZ07_val = split_train_val(inputZ07_path)\n",
    "\n",
    "targetC01, targetC01_val = split_train_val(targetC01_path)\n",
    "targetC02, targetC02_val = split_train_val(targetC02_path)\n",
    "targetC03, targetC03_val = split_train_val(targetC03_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans_val = Compose(\n",
    "    [\n",
    "        LoadImage(PILReader(), image_only=True),\n",
    "        AddChannel(),\n",
    "        CenterSpatialCrop(roi_size=256),\n",
    "        ToTensor()\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = FlowArrayDataset.FlowArrayDataset(\n",
    "    inputZ01=inputZ01_val, inputZ01_transform=trans_val,\n",
    "    inputZ02=inputZ02_val, inputZ02_transform=trans_val,\n",
    "    inputZ03=inputZ03_val, inputZ03_transform=trans_val,\n",
    "    inputZ04=inputZ04_val, inputZ04_transform=trans_val,\n",
    "    inputZ05=inputZ05_val, inputZ05_transform=trans_val,\n",
    "    inputZ06=inputZ06_val, inputZ06_transform=trans_val,\n",
    "    inputZ07=inputZ07_val, inputZ07_transform=trans_val,\n",
    "    targetC01=targetC01_val, targetC01_transform=trans_val,\n",
    "    targetC02=targetC02_val, targetC02_transform=trans_val,\n",
    "    targetC03=targetC03_val, targetC03_transform=trans_val\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=1\n",
    "    #num_workers=4 #multiprocessing.cpu_count(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GeneratorUnet(\n",
       "  (inc): DoubleConv(\n",
       "    (double_conv): Sequential(\n",
       "      (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LeakyReLU(negative_slope=0.1)\n",
       "      (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): LeakyReLU(negative_slope=0.1)\n",
       "    )\n",
       "  )\n",
       "  (down1): Down(\n",
       "    (pool_conv): Sequential(\n",
       "      (0): AvgPool2d(kernel_size=3, stride=2, padding=0)\n",
       "      (1): DoubleConv(\n",
       "        (double_conv): Sequential(\n",
       "          (0): Conv2d(224, 448, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): LeakyReLU(negative_slope=0.1)\n",
       "          (3): Conv2d(448, 448, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (5): LeakyReLU(negative_slope=0.1)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (down2): Down(\n",
       "    (pool_conv): Sequential(\n",
       "      (0): AvgPool2d(kernel_size=3, stride=2, padding=0)\n",
       "      (1): DoubleConv(\n",
       "        (double_conv): Sequential(\n",
       "          (0): Conv2d(448, 896, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): BatchNorm2d(896, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): LeakyReLU(negative_slope=0.1)\n",
       "          (3): Conv2d(896, 896, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): BatchNorm2d(896, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (5): LeakyReLU(negative_slope=0.1)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (down3): Down(\n",
       "    (pool_conv): Sequential(\n",
       "      (0): AvgPool2d(kernel_size=3, stride=2, padding=0)\n",
       "      (1): DoubleConv(\n",
       "        (double_conv): Sequential(\n",
       "          (0): Conv2d(896, 1792, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): BatchNorm2d(1792, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): LeakyReLU(negative_slope=0.1)\n",
       "          (3): Conv2d(1792, 1792, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): BatchNorm2d(1792, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (5): LeakyReLU(negative_slope=0.1)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (down4): Down(\n",
       "    (pool_conv): Sequential(\n",
       "      (0): AvgPool2d(kernel_size=3, stride=2, padding=0)\n",
       "      (1): DoubleConv(\n",
       "        (double_conv): Sequential(\n",
       "          (0): Conv2d(1792, 1792, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): BatchNorm2d(1792, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): LeakyReLU(negative_slope=0.1)\n",
       "          (3): Conv2d(1792, 1792, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): BatchNorm2d(1792, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (5): LeakyReLU(negative_slope=0.1)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (up1): Up(\n",
       "    (up): Upsample(scale_factor=2.0, mode=bilinear)\n",
       "    (conv): DoubleConv(\n",
       "      (double_conv): Sequential(\n",
       "        (0): Conv2d(3584, 1792, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): BatchNorm2d(1792, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): LeakyReLU(negative_slope=0.1)\n",
       "        (3): Conv2d(1792, 896, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (4): BatchNorm2d(896, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): LeakyReLU(negative_slope=0.1)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (up2): Up(\n",
       "    (up): Upsample(scale_factor=2.0, mode=bilinear)\n",
       "    (conv): DoubleConv(\n",
       "      (double_conv): Sequential(\n",
       "        (0): Conv2d(1792, 896, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): BatchNorm2d(896, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): LeakyReLU(negative_slope=0.1)\n",
       "        (3): Conv2d(896, 448, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (4): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): LeakyReLU(negative_slope=0.1)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (up3): Up(\n",
       "    (up): Upsample(scale_factor=2.0, mode=bilinear)\n",
       "    (conv): DoubleConv(\n",
       "      (double_conv): Sequential(\n",
       "        (0): Conv2d(896, 448, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): LeakyReLU(negative_slope=0.1)\n",
       "        (3): Conv2d(448, 224, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (4): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): LeakyReLU(negative_slope=0.1)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (up4): Up(\n",
       "    (up): Upsample(scale_factor=2.0, mode=bilinear)\n",
       "    (conv): DoubleConv(\n",
       "      (double_conv): Sequential(\n",
       "        (0): Conv2d(448, 224, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): LeakyReLU(negative_slope=0.1)\n",
       "        (3): Conv2d(224, 224, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (4): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): LeakyReLU(negative_slope=0.1)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (outc): OutConv(\n",
       "    (conv): Conv2d(224, 1, kernel_size=(1, 1), stride=(1, 1))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "netG = GeneratorUnet().to(device)\n",
    "netG.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_weight_dir = \"checkpoints/tempcheckpoint/G_epoch_700.pth\"\n",
    "save_dir = \"testresults/temptestresults\"\n",
    "\n",
    "\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint: checkpoints/tempcheckpoint/G_epoch_700.pth\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f'Loading checkpoint: {load_weight_dir}')\n",
    "path_to_saved_weight = os.path.join(load_weight_dir)\n",
    "checkpoint = torch.load(path_to_saved_weight)  # when you are loading weights saved on gpu device\n",
    "netG.load_state_dict(checkpoint['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 63/63 [00:19<00:00,  3.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SSIM: 0.02202738087535614 PSNR: -69.68706780364853\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "PSNRs = []\n",
    "SSIMs = []\n",
    "\n",
    "mseloss = nn.MSELoss()\n",
    "    \n",
    "with torch.no_grad():\n",
    "    for batch_index, batch in enumerate(tqdm(validation_loader)):\n",
    "        inputZ01, inputZ02, inputZ03, inputZ04, inputZ05, inputZ06, inputZ07 = \\\n",
    "            batch[0].to(device), batch[1].to(device), batch[2].to(device),\\\n",
    "            batch[3].to(device), batch[4].to(device), batch[5].to(device), batch[6].to(device)\n",
    "        \n",
    "        targetC01, targetC02, targetC03 = batch[7].to(device), batch[8].to(device), batch[9].to(device)\n",
    "        \n",
    "        # now compute output\n",
    "        outputC01, outputC02, outputC03 = netG(inputZ01, inputZ02, inputZ03, inputZ04, inputZ05, inputZ06, inputZ07)\n",
    "        \n",
    "        # now compute loss/metric\n",
    "        lossC01 = mseloss(outputC01, targetC01)\n",
    "        lossC02 = mseloss(outputC02, targetC02)\n",
    "        lossC03 = mseloss(outputC03, targetC03)\n",
    "        content_loss = lossC01 + lossC02 + lossC03\n",
    "        \n",
    "        outputCs = torch.cat((outputC01, outputC02, outputC03), dim=1)\n",
    "        targetCs = torch.cat((targetC01, targetC02, targetC03), dim=1)\n",
    "        \n",
    "        psnr = 10 * log10(1 / nn.MSELoss()(outputCs, targetCs))\n",
    "        ssim = pytorch_ssim.ssim(outputCs, targetCs)\n",
    "        \n",
    "        PSNRs.append(psnr)\n",
    "        SSIMs.append(ssim.item())\n",
    "        \n",
    "        # now save each channels as tif\n",
    "        outputC01_image = ToPILImage()(outputC01[0].data.cpu())\n",
    "        outputC01_image.save(os.path.join(save_dir,targetC01_val[batch_index][17:]))\n",
    "        \n",
    "        outputC02_image = ToPILImage()(outputC02[0].data.cpu())\n",
    "        outputC02_image.save(os.path.join(save_dir,targetC02_val[batch_index][17:]))\n",
    "        \n",
    "        outputC03_image = ToPILImage()(outputC03[0].data.cpu())\n",
    "        outputC03_image.save(os.path.join(save_dir,targetC03_val[batch_index][17:]))\n",
    "        \n",
    "        # now save RGB image as tiff \n",
    "        \n",
    "        targetCs_name = f'targetCs_{batch_index}.png'\n",
    "        outputCs_name = f'outputCs_{batch_index}.png'\n",
    "        \n",
    "        outputCs_image = ToPILImage()(outputCs[0].data.cpu())\n",
    "        outputCs_image.save(os.path.join(save_dir,outputCs_name))\n",
    "        \n",
    "        targetCs_image = ToPILImage()(targetCs[0].data.cpu())\n",
    "        targetCs_image.save(os.path.join(save_dir,targetCs_name))\n",
    "        \n",
    "        \n",
    "        # now save RGB image as tiff \n",
    "        #outputCs_save = outputCs.squeeze(0).detach().cpu().numpy() # killing batch dimension\n",
    "        #targetCs_save = targetCs.squeeze(0).detach().cpu().numpy()\n",
    "        \n",
    "        #outputCs_save = np.moveaxis(outputCs_save, 0, -1) # channel frist to channel last\n",
    "        #targetCs_save = np.moveaxis(targetCs_save, 0, -1)\n",
    "        \n",
    "    ssim_mean = np.array(SSIMs).mean()\n",
    "    psnr_mean = np.array(PSNRs).mean()\n",
    "    \n",
    "    print(\"SSIM:\", ssim_mean, \"PSNR:\", psnr_mean)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
